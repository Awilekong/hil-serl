# SERLè®­ç»ƒé€»è¾‘è¯¦ç»†æ€»ç»“

## ğŸ“‹ ç›®å½•
1. [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
2. [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
3. [ç­–ç•¥ç½‘ç»œ](#ç­–ç•¥ç½‘ç»œ)
4. [å¥–åŠ±ä¸Reseté€»è¾‘](#å¥–åŠ±ä¸reseté€»è¾‘)
5. [æ•°æ®æµ](#æ•°æ®æµ)
6. [å…³é”®å‚æ•°](#å…³é”®å‚æ•°)

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„

### åŒè¿›ç¨‹æ¶æ„ï¼šLearner + Actor

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      è®­ç»ƒç³»ç»Ÿæ¶æ„                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         ç½‘ç»œå‚æ•°          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Learnerè¿›ç¨‹    â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  Actor   â”‚ â”‚
â”‚  â”‚  (run_learner)   â”‚                          â”‚ è¿›ç¨‹     â”‚ â”‚
â”‚  â”‚                  â”‚                          â”‚(run_actor)â”‚ â”‚
â”‚  â”‚ - ä¸è¿æœºå™¨äºº      â”‚                          â”‚          â”‚ â”‚
â”‚  â”‚ - GPUè®­ç»ƒ        â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ - è¿æœºå™¨äººâ”‚ â”‚
â”‚  â”‚ - æ›´æ–°ç½‘ç»œ        â”‚      å‘é€æ–°ç­–ç•¥å‚æ•°        â”‚ - CPUæ‰§è¡Œâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                                         â”‚       â”‚
â”‚           â”‚                                         â”‚       â”‚
â”‚      è¯»å–Demo                                   é‡‡é›†æ–°æ•°æ®   â”‚
â”‚           â”‚                                         â”‚       â”‚
â”‚           â–¼                                         â–¼       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Demo Buffer     â”‚                    â”‚ Replay Bufferâ”‚  â”‚
â”‚  â”‚  (50%æ•°æ®)       â”‚                    â”‚  (50%æ•°æ®)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ è®­ç»ƒæµç¨‹

### 1. Learnerè¿›ç¨‹ (`run_learner.sh`)

**å¯åŠ¨å‘½ä»¤ï¼š**
```bash
python train_rlpd.py \
    --exp_name=ram_insertion \
    --checkpoint_path=charge_first_run \
    --demo_path=<demo1.pkl> \
    --demo_path=<demo2.pkl> \
    --learner
```

**ä¸»è¦èŒè´£ï¼š**

```python
# 1ï¸âƒ£ åˆå§‹åŒ–
config = CONFIG_MAPPING["ram_insertion"]()  # åŠ è½½é…ç½®
env = config.get_environment(
    fake_env=True,        # â† å…³é”®ï¼ä¸è¿æ¥çœŸå®æœºå™¨äºº
    classifier=True       # åŠ è½½reward classifier
)

# 2ï¸âƒ£ åŠ è½½Demoæ•°æ®åˆ°demo_buffer
demo_buffer = load_demos(FLAGS.demo_path)

# 3ï¸âƒ£ åˆ›å»ºTrainerServerï¼Œç­‰å¾…actorè¿æ¥
server = TrainerServer()
server.register_data_store("actor_env", replay_buffer)
server.register_data_store("actor_env_intvn", demo_buffer)
server.start()

# 4ï¸âƒ£ ç­‰å¾…replay_bufferå¡«å……åˆ°training_startsï¼ˆé»˜è®¤5000æ¡transitionsï¼‰
while len(replay_buffer) < config.training_starts:
    time.sleep(1)

# 5ï¸âƒ£ å‘é€åˆå§‹ç½‘ç»œå‚æ•°ç»™actor
server.publish_network(agent.state.params)

# 6ï¸âƒ£ è®­ç»ƒå¾ªç¯ (max_stepsæ¬¡è¿­ä»£)
for step in range(config.max_steps):
    # 50/50é‡‡æ ·ï¼šä¸€åŠæ¥è‡ªdemoï¼Œä¸€åŠæ¥è‡ªåœ¨çº¿æ•°æ®
    batch_online = next(replay_buffer.iterator)      # 50% åœ¨çº¿æ•°æ®
    batch_demo = next(demo_buffer.iterator)          # 50% Demoæ•°æ®
    batch = concat([batch_online, batch_demo])
    
    # æ›´æ–°ç­–ç•¥ç½‘ç»œ
    agent, info = agent.update(
        batch,
        networks_to_update=["critic", "actor", "temperature"]
    )
    
    # æ¯steps_per_updateæ­¥å‘é€æ–°å‚æ•°ç»™actor
    if step % config.steps_per_update == 0:
        server.publish_network(agent.state.params)
    
    # æ¯checkpoint_periodæ­¥ä¿å­˜checkpoint
    if step % config.checkpoint_period == 0:
        save_checkpoint(agent.state, step)
```

---

### 2. Actorè¿›ç¨‹ (`run_actor.sh`)

**å¯åŠ¨å‘½ä»¤ï¼š**
```bash
python train_rlpd.py \
    --exp_name=ram_insertion \
    --checkpoint_path=charge_first_run \
    --actor
```

**ä¸»è¦èŒè´£ï¼š**

```python
# 1ï¸âƒ£ åˆå§‹åŒ–
env = config.get_environment(
    fake_env=False,       # â† å…³é”®ï¼è¿æ¥çœŸå®æœºå™¨äºº
    classifier=True       # åŠ è½½reward classifierç”¨äºè®¡ç®—reward
)

# 2ï¸âƒ£ è¿æ¥åˆ°learner
client = TrainerClient(
    "actor_env",
    FLAGS.ip,  # learnerçš„IPåœ°å€
    data_stores={"actor_env": data_store, "actor_env_intvn": intvn_data_store}
)

# 3ï¸âƒ£ æ³¨å†Œç½‘ç»œå‚æ•°æ›´æ–°å›è°ƒ
def update_params(params):
    agent = agent.replace(state=agent.state.replace(params=params))

client.recv_network_callback(update_params)

# 4ï¸âƒ£ æ¢ç´¢å¾ªç¯ (max_stepsæ¬¡)
obs, _ = env.reset()
for step in range(config.max_steps):
    # å‰random_stepsæ­¥éšæœºæ¢ç´¢
    if step < config.random_steps:
        actions = env.action_space.sample()
    else:
        # ä½¿ç”¨ç­–ç•¥ç½‘ç»œé‡‡æ ·åŠ¨ä½œ
        actions = agent.sample_actions(
            observations=obs,
            argmax=False,  # éšæœºé‡‡æ ·ï¼Œç”¨äºæ¢ç´¢
            seed=rng_key
        )
    
    # æ‰§è¡ŒåŠ¨ä½œ
    next_obs, reward, done, truncated, info = env.step(actions)
    
    # æ£€æŸ¥äººç±»å¹²é¢„
    if "intervene_action" in info:
        actions = info["intervene_action"]  # ä½¿ç”¨äººç±»åŠ¨ä½œ
        intvn_data_store.insert(transition)  # å­˜å…¥demo buffer
    
    # å­˜å‚¨transition
    transition = {
        "observations": obs,
        "actions": actions,
        "next_observations": next_obs,
        "rewards": reward,
        "masks": 1.0 - done,
        "dones": done
    }
    data_store.insert(transition)
    
    obs = next_obs
    
    # â­ å…³é”®ï¼å¦‚æœdoneæˆ–truncatedï¼Œresetç¯å¢ƒ
    if done or truncated:
        client.update()  # å‘é€ç»Ÿè®¡ä¿¡æ¯ç»™learner
        obs, _ = env.reset()  # â† é‡ç½®ç¯å¢ƒï¼Œå¼€å§‹æ–°episode
```

---

## ğŸ¯ ç­–ç•¥ç½‘ç»œ (Policy Network)

### è¾“å…¥ (Observations)

ç­–ç•¥ç½‘ç»œæ¥æ”¶çš„è§‚å¯ŸåŒ…å«ï¼š

```python
observations = {
    # å›¾åƒè¾“å…¥ (æ ¹æ®config.image_keys)
    "wrist_1": np.array(shape=(1, H, W, 3)),      # æ‰‹è…•ç›¸æœº1
    "wrist_2": np.array(shape=(1, H, W, 3)),      # æ‰‹è…•ç›¸æœº2
    "side_policy": np.array(shape=(1, H, W, 3)),  # ä¾§é¢ç›¸æœº
    
    # çŠ¶æ€è¾“å…¥ (æ ¹æ®config.proprio_keys)
    "state": np.array(shape=(1, state_dim)),
    # stateåŒ…å«: [tcp_pose(6), tcp_force(3), tcp_torque(3)] = 12ç»´
    # è¯¦ç»†: [x, y, z, roll, pitch, yaw, fx, fy, fz, tx, ty, tz]
}
```

**å…³é”®è¯´æ˜ï¼š**
- æ‰€æœ‰è¾“å…¥éƒ½æœ‰batchç»´åº¦ `(1, ...)`ï¼Œå› ä¸ºChunkingWrapperæ·»åŠ äº†æ—¶é—´ç»´åº¦
- å›¾åƒç»è¿‡è£å‰ªï¼ˆIMAGE_CROPï¼‰åè¾“å…¥
- çŠ¶æ€æ˜¯å½’ä¸€åŒ–åçš„æœ¬ä½“æ„Ÿå—ä¿¡æ¯

### è¾“å‡º (Actions)

```python
actions = agent.sample_actions(observations, seed=rng_key, argmax=False)
# è¾“å‡ºå½¢çŠ¶: (action_dim,)

# å¯¹äº setup_mode="single-arm-fixed-gripper":
actions.shape = (6,)  # [delta_x, delta_y, delta_z, delta_roll, delta_pitch, delta_yaw]

# å¯¹äº setup_mode="single-arm-learned-gripper":
actions.shape = (7,)  # [...ä¸Šé¢6ä¸ª..., gripper_command]
```

**åŠ¨ä½œç©ºé—´è¯´æ˜ï¼š**
- åŠ¨ä½œæ˜¯**ç›¸å¯¹å¢é‡**ï¼Œä¸æ˜¯ç»å¯¹ä½ç½®
- ç»è¿‡ACTION_SCALEç¼©æ”¾ï¼š`(0.01, 0.06, 1)` â†’ ä½ç½®Â±1cm, æ—‹è½¬Â±3.4Â°
- RelativeFrameåŒ…è£…å™¨å°†ç›¸å¯¹åŠ¨ä½œè½¬æ¢ä¸ºç»å¯¹ç›®æ ‡ä½ç½®
- ç„¶åå‘é€ç»™Frankaæœºå™¨äººçš„impedance controller

### ç­–ç•¥ç½‘ç»œæ¶æ„

```python
# è§†è§‰ç¼–ç å™¨ (encoder_type="resnet-pretrained")
images â†’ ResNet18 (é¢„è®­ç»ƒ) â†’ ç‰¹å¾å‘é‡ (512ç»´)

# çŠ¶æ€ç¼–ç å™¨
state â†’ MLP(256) â†’ ç‰¹å¾å‘é‡ (256ç»´)

# èåˆä¸ç­–ç•¥å¤´
concat([image_features, state_features]) â†’ MLP(1024, 1024) â†’ 
    â†’ mean (action_dim)
    â†’ log_std (action_dim)
    
# è¾“å‡ºåˆ†å¸ƒ
TanhNormal(mean, std) â†’ é‡‡æ ·åŠ¨ä½œ â†’ tanhå‹ç¼©åˆ°[-1, 1]
```

---

## ğŸ å¥–åŠ±ä¸Reseté€»è¾‘

### å¥–åŠ±è®¡ç®—

**æ¥æºï¼šReward Classifier**

```python
# åœ¨config.pyçš„get_environment()ä¸­
classifier = load_classifier_func(
    checkpoint_path="./classifier_ckpt/",
    image_keys=["side_policy"]  # åªç”¨side_policyç›¸æœº
)

def reward_func(obs):
    # ä½¿ç”¨è®­ç»ƒå¥½çš„åˆ†ç±»å™¨åˆ¤æ–­æˆåŠŸ
    logits = classifier(obs)
    prob = sigmoid(logits)
    
    # é˜ˆå€¼åˆ¤æ–­
    reward = 1 if prob > 0.5 else 0
    return reward

env = MultiCameraBinaryRewardClassifierWrapper(env, reward_func)
```

**å¥–åŠ±ç‰¹æ€§ï¼š**
- **ç¨€ç–å¥–åŠ±**ï¼šåªæœ‰ `0` æˆ– `1` ä¸¤ç§å€¼
- `reward=1`ï¼šä»»åŠ¡æˆåŠŸï¼ˆå¦‚RAMæ’å…¥æˆåŠŸï¼‰
- `reward=0`ï¼šä»»åŠ¡è¿›è¡Œä¸­æˆ–å¤±è´¥

### Episodeç»ˆæ­¢æ¡ä»¶ (Done)

```python
# åœ¨franka_env.pyçš„step()æ–¹æ³•ä¸­
done = (
    self.curr_path_length >= self.max_episode_length  # è¶…æ—¶ (é»˜è®¤100æ­¥)
    or reward == 1                                     # ä»»åŠ¡æˆåŠŸ
    or self.terminate                                  # æ‰‹åŠ¨ç»ˆæ­¢
)
```

**ä¸‰ç§ç»ˆæ­¢æƒ…å†µï¼š**

1. **è¶…æ—¶ç»ˆæ­¢** (`MAX_EPISODE_LENGTH=100`)
   - Episodeè¾¾åˆ°100æ­¥è¿˜æœªæˆåŠŸ
   - `done=True`, `reward=0`

2. **æˆåŠŸç»ˆæ­¢** (Classifieråˆ¤å®šä¸ºæˆåŠŸ)
   - Classifierè¾“å‡º`reward=1`
   - `done=True`, `reward=1`
   - ğŸ‰ **æˆåŠŸæ¡ˆä¾‹ï¼**

3. **æ‰‹åŠ¨ç»ˆæ­¢** (ç´§æ€¥åœæ­¢)
   - æŒ‰ä¸‹ç´§æ€¥åœæ­¢æŒ‰é’®
   - `done=True`, `reward=0`

### Reseté€»è¾‘

```python
# åœ¨actorå¾ªç¯ä¸­
if done or truncated:
    # ğŸ“Š å‘é€episodeç»Ÿè®¡ä¿¡æ¯
    stats = {
        "episode_return": running_return,
        "episode_length": curr_path_length,
        "intervention_count": intervention_count,
        "succeed": reward
    }
    client.request("send-stats", stats)
    
    # ğŸ”„ é‡ç½®ç¯å¢ƒ
    obs, _ = env.reset()
    
    # Resetä¼šï¼š
    # 1. ç§»åŠ¨æœºå™¨äººåˆ°RESET_POSE (TARGET_POSE + [0, 0, 0.05, 0, 0.05, 0])
    # 2. å¦‚æœRANDOM_RESET=Trueï¼Œæ·»åŠ éšæœºæ‰°åŠ¨:
    #    - XY: Â±RANDOM_XY_RANGE (0.01m)
    #    - RZ: Â±RANDOM_RZ_RANGE (0.01rad)
    # 3. é‡ç½®episodeè®¡æ•°å™¨
    # 4. è·å–æ–°çš„åˆå§‹è§‚å¯Ÿ
```

**å…³é”®ç‚¹ï¼š**
- âœ… **æ˜¯çš„ï¼reward=1æ—¶ä¼šç«‹å³reset**
- Resetåæœºå™¨äººå›åˆ°èµ·å§‹ä½ç½®é™„è¿‘
- æ¯ä¸ªepisodeç‹¬ç«‹ï¼ŒæˆåŠŸåä¸ä¼šç»§ç»­åœ¨æˆåŠŸçŠ¶æ€ä¸‹æ¢ç´¢

---

## ğŸ“Š æ•°æ®æµ

### Transitionæ ¼å¼

```python
transition = {
    "observations": {
        "wrist_1": image_array,
        "wrist_2": image_array, 
        "side_policy": image_array,
        "state": state_vector
    },
    "actions": action_array,           # (6,) æˆ– (7,)
    "next_observations": {...},        # åŒobservations
    "rewards": float,                  # 0 æˆ– 1
    "masks": float,                    # 1.0 - done
    "dones": bool,                     # True/False
}
```

### æ•°æ®å­˜å‚¨

**ä¸¤ä¸ªBufferï¼š**

1. **Replay Buffer** (åœ¨çº¿æ•°æ®)
   - å­˜å‚¨actoré‡‡é›†çš„æ‰€æœ‰transitions
   - åŒ…æ‹¬ç­–ç•¥æ¢ç´¢çš„æ•°æ®
   - åŒ…æ‹¬äººç±»å¹²é¢„çš„æ•°æ®

2. **Demo Buffer** (Demoæ•°æ®)
   - å­˜å‚¨é¢„å…ˆå½•åˆ¶çš„demoæ•°æ®ï¼ˆä»pklæ–‡ä»¶åŠ è½½ï¼‰
   - å­˜å‚¨actorè¿è¡Œæ—¶äººç±»å¹²é¢„çš„æ•°æ®

### RLPDé‡‡æ ·ç­–ç•¥

```python
# 50/50é‡‡æ ·æ¯”ä¾‹
batch = {
    50% from replay_buffer,  # åœ¨çº¿æ¢ç´¢æ•°æ®
    50% from demo_buffer     # é«˜è´¨é‡demoæ•°æ®
}

# ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ
# 1. Demoæ•°æ®æä¾›é«˜è´¨é‡çš„æˆåŠŸè½¨è¿¹
# 2. åœ¨çº¿æ•°æ®æä¾›æ¢ç´¢å’Œåˆ†å¸ƒè¦†ç›–
# 3. ç»“åˆä¸¤è€…åŠ é€Ÿå­¦ä¹ å’Œæé«˜ç¨³å®šæ€§
```

---

## âš™ï¸ å…³é”®å‚æ•°

### è®­ç»ƒå‚æ•° (TrainConfig)

```python
class TrainConfig:
    # ç½‘ç»œæ¶æ„
    encoder_type = "resnet-pretrained"      # è§†è§‰ç¼–ç å™¨
    setup_mode = "single-arm-fixed-gripper" # åŠ¨ä½œç©ºé—´é…ç½®
    
    # æ•°æ®é…ç½®
    image_keys = ["wrist_1", "wrist_2", "side_policy"]
    classifier_keys = ["side_policy"]       # Classifierç”¨å“ªä¸ªç›¸æœº
    proprio_keys = ["tcp_pose", "tcp_force", "tcp_torque"]
    
    # è®­ç»ƒè¶…å‚æ•°
    batch_size = 256                        # è®­ç»ƒbatchå¤§å°
    max_steps = 100000                      # æœ€å¤§è®­ç»ƒæ­¥æ•°
    random_steps = 300                      # å‰300æ­¥éšæœºæ¢ç´¢
    training_starts = 5000                  # å¼€å§‹è®­ç»ƒå‰éœ€è¦çš„æ•°æ®é‡
    
    # æ›´æ–°é¢‘ç‡
    steps_per_update = 50                   # æ¯50æ­¥å‘é€æ–°å‚æ•°ç»™actor
    checkpoint_period = 50                  # æ¯50æ­¥ä¿å­˜checkpoint
    log_period = 100                        # æ¯100æ­¥è®°å½•æ—¥å¿—
    buffer_period = 1000                    # æ¯1000æ­¥ä¿å­˜bufferåˆ°ç£ç›˜
    
    # å­¦ä¹ ç‡
    cta_ratio = 2                           # Criticæ›´æ–°æ¬¡æ•° / Actoræ›´æ–°æ¬¡æ•°
    discount = 0.99                         # æŠ˜æ‰£å› å­
```

### ç¯å¢ƒå‚æ•° (EnvConfig)

```python
class EnvConfig:
    # ä½å§¿é…ç½®
    TARGET_POSE = [0.497, 0.092, 0.361, 3.102, 0.012, 0.172]
    RESET_POSE = TARGET_POSE + [0, 0, 0.05, 0, 0.05, 0]
    
    # å®‰å…¨é™åˆ¶
    ABS_POSE_LIMIT_LOW = TARGET_POSE - [0.08, 0.06, 0.03, 0.03, 0.3, 0.8]
    ABS_POSE_LIMIT_HIGH = TARGET_POSE + [0.08, 0.06, 0.12, 0.03, 0.3, 0.8]
    
    # åŠ¨ä½œç¼©æ”¾
    ACTION_SCALE = (0.01, 0.06, 1)          # ä½ç½®Â±1cm, æ—‹è½¬Â±3.4Â°
    
    # Episodeé…ç½®
    MAX_EPISODE_LENGTH = 100                # æœ€å¤§æ­¥æ•°
    RANDOM_RESET = True                     # æ˜¯å¦éšæœºreset
    RANDOM_XY_RANGE = 0.01                  # XYéšæœºèŒƒå›´
    RANDOM_RZ_RANGE = 0.01                  # RZéšæœºèŒƒå›´
    
    # ç›¸æœºé…ç½®
    REALSENSE_CAMERAS = {...}               # ç›¸æœºåºåˆ—å·ã€åˆ†è¾¨ç‡ã€æ›å…‰
    IMAGE_CROP = {...}                      # å›¾åƒè£å‰ªlambdaå‡½æ•°
    
    # é˜»æŠ—æ§åˆ¶å‚æ•°
    COMPLIANCE_PARAM = {
        "translational_stiffness": 2000,
        "rotational_stiffness": 150,
        ...
    }
```

---

## ğŸ“ è®­ç»ƒæµç¨‹æ€»ç»“

### å®Œæ•´å¾ªç¯

```
1ï¸âƒ£ å¯åŠ¨Learnerè¿›ç¨‹
   â””â”€ åŠ è½½configå’Œdemoæ•°æ®
   â””â”€ åˆ›å»ºfake_envï¼ˆä¸è¿æœºå™¨äººï¼‰
   â””â”€ ç­‰å¾…replay_bufferå¡«å……

2ï¸âƒ£ å¯åŠ¨Actorè¿›ç¨‹
   â””â”€ åŠ è½½configå’Œclassifier
   â””â”€ åˆ›å»ºçœŸå®envï¼ˆè¿æ¥æœºå™¨äººï¼‰
   â””â”€ è¿æ¥åˆ°Learner

3ï¸âƒ£ Actoræ¢ç´¢å¾ªç¯ (æ¯ä¸ªepisode):
   â”œâ”€ Resetæœºå™¨äººåˆ°èµ·å§‹ä½ç½®
   â”œâ”€ å¾ªç¯æ‰§è¡ŒåŠ¨ä½œ (æœ€å¤š100æ­¥):
   â”‚  â”œâ”€ ä»ç­–ç•¥ç½‘ç»œé‡‡æ ·åŠ¨ä½œ
   â”‚  â”œâ”€ æ‰§è¡ŒåŠ¨ä½œ
   â”‚  â”œâ”€ Classifierè®¡ç®—reward
   â”‚  â”œâ”€ å­˜å‚¨transition
   â”‚  â””â”€ å¦‚æœreward=1æˆ–è¶…æ—¶ â†’ done=True â†’ è·³å‡º
   â””â”€ Resetç¯å¢ƒï¼Œå¼€å§‹æ–°episode

4ï¸âƒ£ Learnerè®­ç»ƒå¾ªç¯:
   â”œâ”€ ä»replay_bufferå’Œdemo_bufferå„é‡‡æ ·50%
   â”œâ”€ æ›´æ–°Criticç½‘ç»œ (2æ¬¡)
   â”œâ”€ æ›´æ–°Actorå’ŒTemperature (1æ¬¡)
   â”œâ”€ æ¯50æ­¥å‘é€æ–°å‚æ•°ç»™Actor
   â””â”€ æ¯50æ­¥ä¿å­˜checkpoint

5ï¸âƒ£ æŒç»­è¿­ä»£ç›´åˆ°max_steps
```

### å…³é”®ç‰¹æ€§

- âœ… **å¼‚æ­¥è®­ç»ƒ**ï¼šLearnerå’ŒActorå¹¶è¡Œè¿è¡Œ
- âœ… **ç¨€ç–å¥–åŠ±**ï¼šåªæœ‰0/1ï¼Œé classifieråˆ¤æ–­æˆåŠŸ
- âœ… **ç«‹å³Reset**ï¼šæˆåŠŸåç«‹å³resetï¼Œä¸ä¼šç»§ç»­æ¢ç´¢æˆåŠŸçŠ¶æ€
- âœ… **äººç±»å¹²é¢„**ï¼šSpaceMouseå¹²é¢„çš„æ•°æ®ä¼šç‰¹åˆ«æ ‡è®°å¹¶å­˜å…¥demo_buffer
- âœ… **50/50é‡‡æ ·**ï¼šå¹³è¡¡demoæ•°æ®å’Œåœ¨çº¿æ•°æ®
- âœ… **ç›¸å¯¹åŠ¨ä½œ**ï¼šç­–ç•¥è¾“å‡ºç›¸å¯¹å¢é‡ï¼Œä¸æ˜¯ç»å¯¹ä½ç½®

---

## ğŸ“ å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆreward=1åè¦resetï¼Ÿ**
A: å› ä¸ºä»»åŠ¡å·²ç»å®Œæˆï¼Œç»§ç»­åœ¨æˆåŠŸçŠ¶æ€ä¸‹æ¢ç´¢æ²¡æœ‰æ„ä¹‰ã€‚Resetåå¼€å§‹æ–°çš„å°è¯•ï¼Œæ”¶é›†æ›´å¤šæ ·åŒ–çš„æ•°æ®ã€‚

**Q: ç­–ç•¥ç½‘ç»œçœ‹åˆ°çš„æ˜¯ä»€ä¹ˆï¼Ÿ**
A: 3ä¸ªç›¸æœºçš„RGBå›¾åƒ + 12ç»´çŠ¶æ€å‘é‡ï¼ˆä½ç½®ã€åŠ›ã€åŠ›çŸ©ï¼‰

**Q: åŠ¨ä½œç©ºé—´æ˜¯ä»€ä¹ˆï¼Ÿ**
A: 6Dç›¸å¯¹å¢é‡ [dx, dy, dz, droll, dpitch, dyaw]ï¼Œç»è¿‡ACTION_SCALEç¼©æ”¾åå‘é€ç»™é˜»æŠ—æ§åˆ¶å™¨

**Q: Classifieråœ¨å“ªé‡Œè®­ç»ƒï¼Ÿ**
A: ä½¿ç”¨`train_reward_classifier.py`å•ç‹¬è®­ç»ƒï¼Œä½¿ç”¨success/failureå›¾åƒæ•°æ®

**Q: ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ä¸ªè¿›ç¨‹ï¼Ÿ**
A: Learnerç”¨GPUé«˜æ•ˆè®­ç»ƒï¼ŒActorç”¨CPUä¸æœºå™¨äººäº¤äº’ã€‚åˆ†ç¦»åäº’ä¸é˜»å¡ï¼Œæé«˜æ•ˆç‡ã€‚

---

ç”Ÿæˆæ—¶é—´: 2025-11-27
æ–‡ä»¶ä½ç½®: `/home/dexfranka/ws_zpw/hil-serl/examples/TRAINING_LOGIC_SUMMARY.md`
